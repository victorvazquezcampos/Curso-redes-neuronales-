{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4tXE4PbErOYtJdpmZRZ+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorvazquezcampos/Curso-redes-neuronales-/blob/main/NRODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h0H89eUEY_eu"
      },
      "outputs": [],
      "source": [
        "# librerias \n",
        "# paqueterias\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Nos basamos en el codigo visto en clase de la soluci√≥n de EDO\n",
        "\n",
        "class ODEsolver(Sequential):\n",
        "   loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "   class ODEsolver(Sequential):\n",
        "    loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def train_step(self, data):\n",
        "        batch_size = tf.shape(data)[0]\n",
        "        x = tf.random.uniform((batch_size, 1), minval=-5, maxval=5)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            with tf.GradientTape() as tape2:\n",
        "                tape2.watch(x)\n",
        "                y_pred = self(x, training=True)\n",
        "            dy = tape2.gradient(y_pred, x)\n",
        "            x_0 = tf.zeros((batch_size, 1))\n",
        "            y_0 = self(x_0, training=True)\n",
        "            eq = x * dy + y_pred - x ** 2 * keras.backend.cos(x)\n",
        "            ic = y_0\n",
        "            loss = keras.losses.mean_squared_error(0., eq) + keras.losses.mean_squared_error(0., ic)\n",
        "\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "        @property\n",
        "        def metrics(self):\n",
        "            return [keras.metrics.Mean(name='loss')]\n",
        "\n",
        "\n"
      ]
    }
  ]
}